{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to perform evaluation on 2DSyn_BC dataset\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import torch\n",
    "import tifffile\n",
    "import random\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "import data\n",
    "from data.sampler.synBC2D_sampler import SynBC2DSampler\n",
    "from options.options import Options\n",
    "from util.mesh_handler import *\n",
    "from util.util import *\n",
    "from metric_funcs import *\n",
    "\n",
    "from models.biospade_model import BioSPADEModel\n",
    "import models.networks as networks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_patches = 5 #200 # Number of patches to test\n",
    "n_instances = 5 #25 # Number of instances per patch to test\n",
    "name = 'Syn2D_BC' # Name of experiments\n",
    "\n",
    "# Generate Real Data\n",
    "def generate(vox, powers, frames, blur=True, std=None, n_instances=1, rand_noise=True):\n",
    "    if len(powers) != len(frames):\n",
    "        raise ValueError('Powers and Frames are not equal in length')\n",
    "    \n",
    "    if blur:\n",
    "        vox = blur_vox(vox, org_opt.sigmas)\n",
    "\n",
    "    bin_mat = None\n",
    "    p = org_opt.p_blobs\n",
    "    real = np.zeros([n_instances, len(powers), *vox.shape[-2:]])\n",
    "    for j in range(len(powers)):\n",
    "        if not rand_noise:\n",
    "            bin_mat = np.random.choice([0.0, 1.0], size=vox.shape, p=[1-p,p])\n",
    "        for i in range(n_instances):\n",
    "            real[i, j] = sampler.generate(vox, powers[j], frames[j], std_scalar=std, bin_mat=bin_mat)\n",
    "    return real\n",
    "\n",
    "# Generate noise for each layer of G\n",
    "def generate_noise(sz):\n",
    "    xy_sz = sz[-2:]\n",
    "    \n",
    "    noise = []\n",
    "    for i in range(num_up_layers+1):\n",
    "        x_sz = xy_sz[0]//(2**i)\n",
    "        y_sz = xy_sz[1]//(2**i)\n",
    "\n",
    "        noise += [torch.randn([*sz[:2],x_sz, y_sz]).cuda()]\n",
    "    return noise\n",
    "\n",
    "# Generate fake images\n",
    "def forward(model, vox, powers, frames, n_instances=1, rand_noise=True, is_cycle=False):\n",
    "    if len(powers) != len(frames):\n",
    "        raise ValueError('Powers and Frames are not equal in length')\n",
    "\n",
    "    fake = torch.zeros([n_instances, len(powers),*vox.shape[-2:]])\n",
    "    fake_mu = torch.zeros_like(fake)\n",
    "    noise = generate_noise([len(powers),*vox.shape[-3:]])\n",
    "    for i in range(n_instances):\n",
    "        data = {'mesh_semantics': len(powers)*[vox],\n",
    "                'power': powers,\n",
    "                'frames': frames,\n",
    "                'z_pos': len(powers)*[0]}\n",
    "        data = tensorize_dict(data)\n",
    "        if not rand_noise:\n",
    "            data['noise'] = noise\n",
    "            \n",
    "        if is_cycle:\n",
    "            model.set_input(data)\n",
    "            fake_ = model.forward(True)\n",
    "            fake[i] = torch.relu(fake_[:,0])\n",
    "        else:\n",
    "            fake_, fake_mu_ = model(data,'inference')\n",
    "            fake[i], fake_mu[i] = fake_[:,0,0], fake_mu_[:,0,0]\n",
    "    return fake, fake_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack dataset [SynBC2DDataset] of size 41 was created\n",
      "mesh dataset [SynBC2DDataset] of size 41 was created\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters\n",
    "\n",
    "tag = 'synBC2D'\n",
    "org_opt = Options('options/test_options.yaml', tag)\n",
    "org_opt.train_mode = 'GAN'\n",
    "org_opt.initialize()\n",
    "\n",
    "org_opt.dataset_mode = tag\n",
    "org_opt.name = name\n",
    "org_opt.how_many_patches = n_patches\n",
    "\n",
    "org_opt.samples_per_instance = 1\n",
    "\n",
    "num_up_layers = networks.generator.compute_latent_vector_size(org_opt)\n",
    "\n",
    "dataloader, dataset = data.create_dataloader(org_opt, 'all')\n",
    "sampler = SynBC2DSampler(org_opt)\n",
    "files = dataset.mesh_paths\n",
    "\n",
    "style_combs = np.asarray(list(itertools.product(org_opt.powers, org_opt.frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing data\n",
    "\n",
    "test_input = np.zeros([org_opt.how_many_patches, 1, *org_opt.crop_xy_sz])\n",
    "real_input = np.zeros([org_opt.how_many_patches, 1, *org_opt.crop_xy_sz])\n",
    "test_gt = np.zeros([org_opt.how_many_patches, *org_opt.crop_xy_sz])\n",
    "real_patches = np.zeros([org_opt.how_many_patches, n_instances, len(style_combs), *org_opt.crop_xy_sz])\n",
    "\n",
    "n_samples = 0\n",
    "while n_samples<org_opt.how_many_patches:\n",
    "    batch = next(iter(dataloader))\n",
    "    input = batch['mesh_semantics'].view(-1,1,*org_opt.crop_xy_sz).numpy()\n",
    "    real_input_ = batch['real_semantics'].view(-1,1,*org_opt.crop_xy_sz).numpy()\n",
    "    \n",
    "    n_input = min([len(input), org_opt.how_many_patches-n_samples])\n",
    "    test_input[n_samples:n_samples+n_input] = input[:n_input]\n",
    "    real_input[n_samples:n_samples+n_input] = real_input_[:n_input]\n",
    "    \n",
    "    n_samples += n_input\n",
    "\n",
    "for i, input in enumerate(test_input):\n",
    "    real_patches[i] = generate(input, style_combs[:,0], style_combs[:,1].astype(int), n_instances=n_instances, rand_noise=True)\n",
    "    test_gt[i] = uniform_filter(input, 3)>1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Syn2D_BC\n",
      "Network [SPADEGenerator] was created. Total number of parameters: 8.3 million. To see the architecture, do print(network).\n"
     ]
    }
   ],
   "source": [
    "# Perform Evaluation\n",
    "\n",
    "out_sz = (org_opt.number_of_experiments, len(style_combs))\n",
    "losses = {'NMSE': np.zeros(out_sz),\n",
    "          'PSNR': np.zeros(out_sz),\n",
    "          'JSD': np.zeros(out_sz),\n",
    "          'GLCM': np.zeros(out_sz),\n",
    "          'LBP': np.zeros(out_sz)}\n",
    "\n",
    "real = real_patches.swapaxes(0,2)\n",
    "avg_real = real.mean(1)\n",
    "blur_real = blur_all(real)\n",
    "text_real = blur_real[:,0]\n",
    "\n",
    "real_PSNR = calc_PSNR(real, axis=(1,2,3,4)) # Compute real PSNR\n",
    "real_LBP = calc_LBP(text_real[:,:,None]) # Compute real LBP\n",
    "real_GLCM = calc_GLCM(text_real[:,:,None]) # Compute real GLCM\n",
    "\n",
    "for exp in range(0, org_opt.number_of_experiments):\n",
    "    fake_patches = np.zeros([org_opt.how_many_patches, n_instances, len(style_combs), *org_opt.crop_xy_sz])\n",
    "\n",
    "    # Get experiment details\n",
    "    opt = deepcopy(org_opt)\n",
    "    exp_str = ''\n",
    "    if not org_opt.run_all:\n",
    "        exp_str = '(exp'+str(exp)+')'\n",
    "        opt.set_experiment(exp)\n",
    "    opt.name = org_opt.name+exp_str\n",
    "\n",
    "    # Load model\n",
    "    print('Evaluating:', opt.name)\n",
    "    model = BioSPADEModel(opt)\n",
    "    model.eval()\n",
    "\n",
    "    # Generate data\n",
    "    for i, input in enumerate(test_input):\n",
    "        fake_patches[i],_  = forward(model, input[:,None], \n",
    "                                     style_combs[:,0], style_combs[:,1], \n",
    "                                     n_instances=n_instances)\n",
    "        \n",
    "    fake = fake_patches.swapaxes(0,2)\n",
    "    avg_fake = fake.mean(1)\n",
    "    text_fake = blur_all(fake)[:,0]\n",
    "    \n",
    "    fake_LBP = calc_LBP(text_fake[:,:,None])\n",
    "    fake_GLCM = calc_GLCM(text_fake[:,:,None])\n",
    "    \n",
    "    losses['NMSE'][exp] = MSE(avg_fake*test_gt, avg_real*test_gt, normalize=True) # Compute NMSE\n",
    "    losses['GLCM'][exp] = MSE(fake_GLCM, real_GLCM,axis=(1,2,3)) # Compute MSE of GLCM\n",
    "    losses['LBP'][exp] = mult_JSDs(fake_LBP, real_LBP) # Compute JSD of LBP\n",
    "    losses['PSNR'][exp] = (calc_PSNR(fake, axis=(1,2,3,4))-real_PSNR)**2 # Compute MSE of PSNR\n",
    "    losses['JSD'][exp] = compare_hist(fake, real) # Compute JSD of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSE & LBP & GLCM & PSNR & JSD & \n",
      "EXP 1: 1.594 & 3.687 & 0.061 & 0.003 & 0.017 & \n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "\n",
    "keys = ['NMSE', 'LBP', 'GLCM', 'PSNR', 'JSD']\n",
    "scale = {'NMSE': 1e1, 'GLCM': 1e8, 'LBP': 1e3, 'PSNR':1e0, 'JSD': 1e2}\n",
    "\n",
    "ln = ''\n",
    "for key in keys:\n",
    "    ln += key+' & '\n",
    "print(ln)\n",
    "\n",
    "for i_loss in range(org_opt.number_of_experiments):\n",
    "    ln = 'EXP '+str(i_loss+1)+': ' # Set to '' if you want to be able to copy and past table into excel\n",
    "    for key in keys:\n",
    "        val = losses[key][i_loss].mean()\n",
    "        ln += '%0.3f & ' % (val*scale[key])\n",
    "    print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
